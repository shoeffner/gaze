meta:
  # relative target grid on the screen
  target:
    width: 8
    height: 5
  camera:
    # In m, from upper left screen corner
    position:
      x: 0.1725
      y: -0.007
    # Resolution to use
    resolution:
      width: 640
      height: 360
    # Frames per second
    fps: 30
    # The sensor size is tricky to get, as it is not always reported.
    # In fact, https://www.cnet.com/products/apple-isight/specs/ reports
    # this size for the 640x480 iSight, while in this file we use a more modern iSight
    # with 1280x720.
    # In m.
    sensor_size: 0.00635
    sensor_aspect_ratio: 16:9
    # The focal length can be measured using the angle of view:
    # https://en.wikipedia.org/wiki/Angle_of_view
    focal_length: 0.01
    # The camera matrix can be created using OpenCV's calibration tool.
    # https://github.com/opencv/opencv/blob/fc9e031454fd456d09e15944c99a419e73d80661/samples/cpp/calibration.cpp
    # In case you do not calibrate it, you can either leave out this section,
    # or set the data to:
    # [ SourceCaptureWidth, 0, SourceCaptureWidth / 2,
    #   0, SourceCaptureWidth, SourceCaptureHeight / 2,
    #   0, 0, 1 ]
    # as an approximation.
    # Note that you should calibrate your camera for the size you supply in
    # SourceCapture!
    camera_matrix: !!opencv-matrix
       rows: 3
       cols: 3
       dt: d
       data: [640, 0, 320, 0, 640, 180, 0, 0, 1]
    # The distortion coefficients can be created using OpenCV's calibration tool.
    # https://github.com/opencv/opencv/blob/fc9e031454fd456d09e15944c99a419e73d80661/samples/cpp/calibration.cpp
    # In case you do not calibrate it, just use a 4x1 matrix filled with zeros.
    # Note that during calibration, you might also get a 5x1 matrix or others,
    # don't worry!
    distortion_coefficients: !!opencv-matrix
       rows: 4
       cols: 1
       dt: d
       data: [0, 0, 0, 0]
  screen:
    resolution:
      # in pixels
      width: 2880
      height: 1800
    measurements:
      # in m
      width: 0.335
      height: 0.207
pipeline:
  - type: SourceCapture
    # This should be a number for webcams, or a path to an image or video file
    source: 0
  - type: FaceLandmarks
    name: FaceLandmarks68
    model: shape_predictor_68_face_landmarks.dat
  - type: HeadPoseEstimation
    name: HeadPose68
    # See https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/
    # (Here the indices are 0-based, so remember to take care of the offset:
    #  0 here is 1 in the ibug figure etc.)
    landmark_indices: [30, 8, 36, 45, 48, 54]
    model: [
      [0.0, 0.0, 0.0],  # Nosetip (Pronasal prn)
      [0.0, -0.0636, -0.0125],  # Chin tip (Gnathion/Menton gn)
      [-0.0433, 0.0327, -0.026],  # Outside right eye (right exocanthion ex_r)
      [0.0433, 0.0327, -0.026],  # Outside left eye (left exocanthion ex_l)
      [-0.0289, -0.0289, -0.0241],  # Right mouth corner (Cheilion ch_r)
      [0.0289, -0.0289, -0.0241]  # Left mouth corner (Cheilion ch_l)
    ]
    model_scale: 1
# An alternative approach for the pupil localization with fixed running times
# and similar results is EyeLike. It scales detected eyes to a width of 50 px.
# This way it is faster for subjects sitting close to the camera, but slower
# for subjects sitting far away from the camera. It is similarly accurate
# as the PupilLocalization is, as both essentially use the same algorithm.
#  - type: EyeLike
#    relative_threshold: 0.3
  - type: PupilLocalization
    relative_threshold: 0.3
  - type: GazePointCalculation
    eye_ball_centers: [
      [-0.029205, 0.0327, -0.0395],  # Right eye ball center
      [0.029205, 0.0327, -0.0395]  # Left eye ball center
    ]
    # TODO(shoeffner): The landmark indices and model are reused
    # from HeadPoseEstimation, the configuration should be moved.
    # Additionally, GazePointCalculation does not take different model scales
    # into account.
    landmark_indices: [30, 8, 36, 45, 48, 54]
    model: [
      [0.0, 0.0, 0.0],
      [0.0, -0.0636, -0.0125],
      [-0.0433, 0.0327, -0.026],
      [0.0433, 0.0327, -0.026],
      [-0.0289, -0.0289, -0.0241],
      [0.0289, -0.0289, -0.0241]
    ]
# An alternative to approach to the whole geometric pipeline is using
# GazeCapture, which is named after the data set used to train the
# iTracker neural network (see http://gazecapture.csail.mit.edu/).
# To use it, configure Gaze using the -DWITH_CAFFE=ON cmake flag before
# compiling it. You will need to link dlib and caffe to the same BLAS
# implementation, it seems to work best if using openblas. That's why
# caffe should be installed using its Makefile, not using CMake, as
# its CMake version does not allow MacOS to link against openblas but
# only Accelerate.
# Note that you might have to use make without specifying parallel jobs,
# as some build dependencies are still experimental and are not yet
# properly resolved in order.
# The settings provided here should be fine if running from the
# build directory after building caffe from source using the WITH_CAFFE
# flag.
# GazeCapture relies on the pipeline steps FaceLandmarks and SourceCapture.
# - type: GazeCapture
#   name: GazeCapture
#   model: models/itracker_deploy.prototxt
#   weights: models/snapshots/itracker25x_iter_92000.caffemodel
#   mean_left: models/mean_images/mean_left_224_new.binaryproto
#   mean_right: models/mean_images/mean_right_224.binaryproto
#   mean_face: models/mean_images/mean_face_224.binaryproto
